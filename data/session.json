{
  "collection_name": "Protein Optimizing",
  "index_collection_name": null,
  "indexed_doc_ids": [],
  "documents": [
    {
      "id": "BX8FQXE8_5S9ELGUY",
      "item_key": "BX8FQXE8",
      "title": "NeuroFold: A Multimodal Approach to Generating Novel Protein Variants in silico",
      "authors": "Keaun Amani, Michael Fish, Matthew Smith, Christian Danve Marco Castroverde",
      "abstract": "The generation of high-performance enzyme variants with desired physicochemical and functional properties presents a formidable challenge in the field of protein engineering. Existing in silico design methods are limited by inadequate training data, insufficient diversity within datasets, and suboptimal sampling techniques. Here, we introduce a novel approach that addresses these limitations and significantly improves the efficiency of generating functional enzyme variants. Using a multimodal approach, NeuroFold can leverage sequence, structural, and homology data during both sampling and discrimination phases, thereby enabling more diverse and informed sampling of the sequence space. Our model demonstrated a 40-fold increase in Spearman rank correlation as compared to large language models (LLMs) such as ESM-1v and empowers the rapid creation of high-quality enzyme variants, such as the β-lactamase variants generated by NeuroFold in this study, which demonstrated increased thermostability and varying levels of activity. This pipeline represents a promising advancement in the field of enzyme engineering, offering a valuable tool for the development of novel enzymes with enhanced performance and desired chemical properties.",
      "publication": null,
      "date": "2024-03-14 2024-03-14",
      "doi": null,
      "tags": [
        "/read",
        "⭐⭐⭐"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/5S9ELGUY/Amani 等 - 2024 - NeuroFold A Multimodal Approach to Generating Novel Protein Variants in silico.pdf",
      "has_pdf": true,
      "pdf_pages": 32,
      "date_added": "2024-03-15T07:55:57",
      "scanned_at": "2026-01-06T09:32:40.883161",
      "pdf_loaded": false
    },
    {
      "id": "QKX8W96L_MIGFKYZF",
      "item_key": "QKX8W96L",
      "title": "Latent-based Directed Evolution accelerated by Gradient Ascent for Protein Sequence Design",
      "authors": "Nhat Khang Ngo, Thanh V. T. Tran, Viet Thanh Duy Nguyen, Truong Son Hy",
      "abstract": "Directed evolution has been the most effective method for protein engineering that optimizes biological functionalities through a resource-intensive process of screening or selecting among a vast range of mutations. To mitigate this extensive procedure, recent advancements in machine learningguided methodologies center around the establishment of a surrogate sequence-function model. In this paper, we propose Latentbased Directed Evolution (LDE), an evolutionary algorithm designed to prioritize the exploration of high-ﬁtness mutants in the latent space. At its core, LDE is a regularized variational autoencoder (VAE), harnessing the capabilities of the state-ofthe-art Protein Language Model (pLM), ESM-2, to construct a meaningful latent space of sequences. From this encoded representation, we present a novel approach for efﬁcient traversal on the ﬁtness landscape, employing a combination of gradientbased methods and directed evolution. Experimental evaluations conducted on eight protein sequence design tasks demonstrate the superior performance of our proposed LDE over previous baseline algorithms. Our implementation is publicly available at https://github.com/HySonLab/LatentDE.",
      "publication": null,
      "date": "2024-04-16 2024-04-16",
      "doi": "10.1101/2024.04.13.589381",
      "tags": [
        "/read",
        "⭐⭐⭐⭐⭐"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/MIGFKYZF/Ngo 等 - 2024 - Latent-based Directed Evolution accelerated by Gradient Ascent for Protein Sequence Design.pdf",
      "has_pdf": true,
      "pdf_pages": 10,
      "date_added": "2024-04-17T12:16:45",
      "scanned_at": "2026-01-06T09:32:40.883193",
      "pdf_loaded": false
    },
    {
      "id": "Y3ESWJBU_HGC6587W",
      "item_key": "Y3ESWJBU",
      "title": "Now What Sequence? Pre-trained Ensembles for Bayesian Optimization of Protein Sequences",
      "authors": "Ziyue Yang, Katarina A. Milas, Andrew D. White",
      "abstract": "Pre-trained models have been transformative in natural language, computer vision, and now protein sequences by enabling accuracy with few training examples. We show how to use pre-trained sequence models in Bayesian optimization to design new protein sequences with minimal labels (i.e., few experiments). Pre-trained models give good predictive accuracy at low data and Bayesian optimization guides the choice of which sequences to test. Pre-trained sequence models also remove the common requirement of having a list of possible experiments. Any sequence can be considered. We show significantly fewer labeled sequences are required for three sequence design tasks, including creating novel peptide inhibitors with AlphaFold. These de novo peptide inhibitors require only sequence information, no known protein-protein structures, and we can predict highly-efficient binders with less than 10 AlphaFold calculations.",
      "publication": null,
      "date": "2022-09-02 2022-09-02",
      "doi": "10.1101/2022.08.05.502972",
      "tags": [
        "⭐⭐⭐⭐⭐"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/HGC6587W/Yang 等 - 2022 - Now What Sequence Pre-trained Ensembles for Bayesian Optimization of Protein Sequences.pdf",
      "has_pdf": true,
      "pdf_pages": 10,
      "date_added": "2024-07-04T08:38:29",
      "scanned_at": "2026-01-06T09:32:40.883201",
      "pdf_loaded": false
    },
    {
      "id": "IAEXBQL9_FVWVWF84",
      "item_key": "IAEXBQL9",
      "title": "Robust Optimization in Protein Fitness Landscapes Using Reinforcement Learning in Latent Space",
      "authors": "Minji Lee, Luiz Felipe Vecchietti, Hyunkyu Jung, Hyun Joo Ro, Meeyoung Cha, Ho Min Kim",
      "abstract": "Proteins are complex molecules responsible for different functions in nature. Enhancing the functionality of proteins and cellular fitness can significantly impact various industries. However, protein optimization using computational methods remains challenging, especially when starting from low-fitness sequences. We propose LatProtRL, an optimization method to efficiently traverse a latent space learned by an encoder-decoder leveraging a large protein language model. To escape local optima, our optimization is modeled as a Markov decision process using reinforcement learning acting directly in latent space. We evaluate our approach on two important fitness optimization tasks, demonstrating its ability to achieve comparable or superior fitness over baseline methods. Our findings and in vitro evaluation show that the generated sequences can reach highfitness regions, suggesting a substantial potential of LatProtRL in lab-in-the-loop scenarios.",
      "publication": null,
      "date": "2024-05-29 2024-05-29",
      "doi": null,
      "tags": [
        "Computer Science - Machine Learning",
        "Quantitative Biology - Biomolecules",
        "Quantitative Biology - Quantitative Methods",
        "⭐⭐⭐⭐⭐"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/FVWVWF84/Lee 等 - 2024 - Robust Optimization in Protein Fitness Landscapes Using Reinforcement Learning in Latent Space.pdf",
      "has_pdf": true,
      "pdf_pages": 15,
      "date_added": "2024-07-18T07:25:35",
      "scanned_at": "2026-01-06T09:32:40.883208",
      "pdf_loaded": false
    },
    {
      "id": "DT95LBV8_YF92F6XQ",
      "item_key": "DT95LBV8",
      "title": "Improving Protein Optimization with Smoothed Fitness Landscapes",
      "authors": "Andrew Kirjner, Jason Yim, Raman Samusevich, Shahar Bracha, Tommi Jaakkola, Regina Barzilay, Ila Fiete",
      "abstract": "The ability to engineer novel proteins with higher fitness for a desired property would be revolutionary for biotechnology and medicine. Modeling the combinatorially large space of sequences is infeasible; prior methods often constrain optimization to a small mutational radius, but this drastically limits the design space. Instead of heuristics, we propose smoothing the fitness landscape to facilitate protein optimization. First, we formulate protein fitness as a graph signal then use Tikunov regularization to smooth the fitness landscape. We find optimizing in this smoothed landscape leads to improved performance across multiple methods in the GFP and AAV benchmarks. Second, we achieve state-of-the-art results utilizing discrete energy-based models and MCMC in the smoothed landscape. Our method, called Gibbs sampling with Graph-based Smoothing (GGS), demonstrates a unique ability to achieve 2.5 fold fitness improvement (with in-silico evaluation) over its training set. GGS demonstrates potential to optimize proteins in the limited data regime. Code: https://github.com/kirjner/GGS",
      "publication": null,
      "date": "2024-03-02 2024-03-02",
      "doi": "10.48550/arXiv.2307.00494",
      "tags": [
        "Computer Science - Machine Learning",
        "Quantitative Biology - Biomolecules",
        "Quantitative Biology - Quantitative Methods",
        "Statistics - Machine Learning"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/YF92F6XQ/Kirjner 等 - 2024 - Improving Protein Optimization with Smoothed Fitness Landscapes.pdf",
      "has_pdf": true,
      "pdf_pages": 19,
      "date_added": "2024-07-18T07:28:27",
      "scanned_at": "2026-01-06T09:32:40.883213",
      "pdf_loaded": false
    },
    {
      "id": "MVZZ2ANS_2HK9X3C6",
      "item_key": "MVZZ2ANS",
      "title": "Plug & play directed evolution of proteins with gradient-based discrete MCMC",
      "authors": "Patrick Emami, Aidan Perreault, Jeffrey Law, David Biagioni, Peter St. John",
      "abstract": "A long-standing goal of machine-learning-based protein engineering is to accelerate the discovery of novel mutations that improve the function of a known protein. We introduce a sampling framework for evolving proteins in silico that supports mixing and matching a variety of unsupervised models, such as protein language models, and supervised models that predict protein function from sequence. By composing these models, we aim to improve our ability to evaluate unseen mutations and constrain search to regions of sequence space likely to contain functional proteins. Our framework achieves this without any model fine-tuning or re-training by constructing a product of experts distribution directly in discrete protein space. Instead of resorting to brute force search or random sampling, which is typical of classic directed evolution, we introduce a fast Markov chain Monte Carlo sampler that uses gradients to propose promising mutations. We conduct in silico directed evolution experiments on wide fitness landscapes and across a range of different pre-trained unsupervised models, including a 650 M parameter protein language model. Our results demonstrate an ability to efficiently discover variants with high evolutionary likelihood as well as estimated activity multiple mutations away from a wild type protein, suggesting our sampler provides a practical and effective new paradigm for machine-learning-based protein engineering.",
      "publication": "Machine Learning: Science and Technology",
      "date": "2023-06-01 2023-06-01",
      "doi": "10.1088/2632-2153/accacd",
      "tags": [],
      "pdf_path": "/Users/tjx/Zotero/storage/2HK9X3C6/Emami 等 - 2023 - Plug & play directed evolution of proteins with gradient-based discrete MCMC.pdf",
      "has_pdf": true,
      "pdf_pages": 21,
      "date_added": "2024-08-06T06:36:43",
      "scanned_at": "2026-01-06T09:32:40.883334",
      "pdf_loaded": false
    },
    {
      "id": "6V4F7HYX_ACWGR7Y8",
      "item_key": "6V4F7HYX",
      "title": "Protein Sequence Design with Batch Bayesian Optimisation",
      "authors": "Chuanjiao Zong",
      "abstract": "Protein sequence design is a challenging problem in protein engineering, which aims to discover novel proteins with useful biological functions. Directed evolution is a widely-used approach for protein sequence design, which mimics the evolution cycle in a laboratory environment and conducts an iterative protocol. However, the burden of laboratory experiments can be reduced by using machine learning approaches to build a surrogate model of the protein landscape and conducting in-silico population selection through model-based fitness prediction. In this paper, we propose a new method based on Batch Bayesian Optimization (Batch BO), a well-established optimization method, for protein sequence design. By incorporating Batch BO into the directed evolution process, our method is able to make more informed decisions about which sequences to select for artificial evolution, leading to improved performance and faster convergence. We evaluate our method on a suite of in-silico protein sequence design tasks and demonstrate substantial improvement over baseline algorithms.",
      "publication": null,
      "date": "2023-03-18 2023-03-18",
      "doi": "10.48550/arXiv.2303.10429",
      "tags": [
        "Computer Science - Machine Learning"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/ACWGR7Y8/Zong - 2023 - Protein Sequence Design with Batch Bayesian Optimisation.pdf",
      "has_pdf": true,
      "pdf_pages": 9,
      "date_added": "2024-08-16T15:01:02",
      "scanned_at": "2026-01-06T09:32:40.883343",
      "pdf_loaded": false
    },
    {
      "id": "WYLWRG6A_P2F2WQXF",
      "item_key": "WYLWRG6A",
      "title": "Antibody Design with Constrained Bayesian Optimization",
      "authors": "Yimeng Zeng, Hunter Elliott, Phillip Maffettone, Peyton Greenside, Osbert Bastani, Jacob R. Gardner",
      "abstract": "In therapeutic antibody design, achieving a balance between optimizing binding affinity subject to multiple constraints, and sequence diversity within a batch for experimental validation presents an important challenge. Contemporary methods often fall short in simultaneously optimizing these attributes, leading to inefficiencies in experimental exploration and validation. In this work, we tackle this problem using the latest developments in constrained latent space Bayesian optimization. Our methodology leverages a deep generative model to navigate the discrete space of potential antibody sequences, facilitating the selection of diverse, high-potential candidates for synthesis. We also propose a novel way of training VAEs that leads to a lower dimensional latent space and achieves excellent performance under the data-constrained setting. We validate our approach *in vitro* by synthesizing optimized antibodies, demonstrating consistently high binding affinities and preserved thermal stability.",
      "publication": null,
      "date": "2024-04-29 2024/04/29",
      "doi": null,
      "tags": [],
      "pdf_path": null,
      "has_pdf": true,
      "pdf_pages": 0,
      "date_added": "2024-08-16T14:56:40",
      "scanned_at": "2026-01-06T09:32:40.883347",
      "pdf_loaded": false
    },
    {
      "id": "G26GLIGC_SJTN2D7L",
      "item_key": "G26GLIGC",
      "title": "A Pareto-optimal compositional energy-based model for sampling and optimization of protein sequences",
      "authors": "Nataša Tagasovska, Nathan C Frey, Andreas Loukas, Isidro Hötzel, Ryan Lewis Kelly, Yan Wu, Arvind Rajpal, Richard Bonneau, Kyunghyun Cho, Stephen Ra, Vladimir Gligorijevic",
      "abstract": "Deep generative models have emerged as a popular machine learning-based approach for inverse design problems in the life sciences. However, these problems often require sampling new designs that satisfy multiple properties of interest in addition to learning the data distribution. This multi-objective optimization becomes more challenging when properties are independent or orthogonal to each other. In this work, we propose a Pareto-compositional energy-based model (pcEBM), a framework that uses multiple gradient descent for sampling new designs that adhere to various constraints in optimizing distinct properties. We demonstrate its ability to learn non-convex Pareto fronts and generate sequences that simultaneously satisfy multiple desired properties across a series of real-world antibody design tasks.",
      "publication": null,
      "date": null,
      "doi": null,
      "tags": [
        "⭐⭐⭐"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/SJTN2D7L/Tagasovska 等 - A Pareto-optimal compositional energy-based model for sampling and optimization of protein sequences.pdf",
      "has_pdf": true,
      "pdf_pages": 10,
      "date_added": "2024-08-16T14:43:48",
      "scanned_at": "2026-01-06T09:32:40.883352",
      "pdf_loaded": false
    },
    {
      "id": "VLGLXRQI_P2QVLNA5",
      "item_key": "VLGLXRQI",
      "title": "BOtied: Multi-objective Bayesian optimization with tied multivariate ranks",
      "authors": "Ji Won Park, Nataša Tagasovska, Michael Maser, Stephen Ra, Kyunghyun Cho",
      "abstract": "Many scientific and industrial applications require the joint optimization of multiple, potentially competing objectives. Multi-objective Bayesian optimization (MOBO) is a sample-efficient framework for identifying Pareto-optimal solutions. At the heart of MOBO is the acquisition function, which determines the next candidate to evaluate by navigating the best compromises among the objectives. In this paper, we show a natural connection between non-dominated solutions and the extreme quantile of the joint cumulative distribution function (CDF). Motivated by this link, we propose the Pareto-compliant CDF indicator and the associated acquisition function, BOtied. BOtied inherits desirable invariance properties of the CDF, and an efficient implementation with copulas allows it to scale to many objectives. Our experiments on a variety of synthetic and realworld problems demonstrate that BOtied outperforms state-of-the-art MOBO acquisition functions while being computationally efficient for many objectives.",
      "publication": null,
      "date": "2024-06-07 2024-06-07",
      "doi": null,
      "tags": [
        "Computer Science - Machine Learning",
        "Statistics - Machine Learning",
        "⭐⭐⭐⭐⭐"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/P2QVLNA5/Park 等 - 2024 - BOtied Multi-objective Bayesian optimization with tied multivariate ranks.pdf",
      "has_pdf": true,
      "pdf_pages": 21,
      "date_added": "2024-08-16T08:56:09",
      "scanned_at": "2026-01-06T09:32:40.883357",
      "pdf_loaded": false
    },
    {
      "id": "MJTEGAIH_7SVKN82C",
      "item_key": "MJTEGAIH",
      "title": "Monte Carlo Thompson sampling-guided design for antibody engineering",
      "authors": "Taro Kakuzaki, Hikaru Koga, Shuuki Takizawa, Shoichi Metsugi, Hirotake Shiraiwa, Zenjiro Sampei, Kenji Yoshida, Hiroyuki Tsunoda, Reiji Teramoto",
      "abstract": "Antibodies are one of the predominant treatment modalities for various diseases. To improve the characteristics of a lead antibody, such as antigen-binding affinity and stability, we conducted comprehensive substitutions and exhaustively explored their sequence space. However, it is practically unfeasible to evaluate all possible combinations of mutations owing to combinatorial explosion when multiple amino acid residues are incorporated. It was recently reported that a machine-learning guided protein engineering approach such as Thompson sampling (TS) has been used to efficiently explore sequence space in the framework of Bayesian optimization. For TS, over-exploration occurs when the initial data are biasedly distributed in the vicinity of the lead antibody. We handle a large-scale virtual library that includes numerous mutations. When the number of experiments is limited, this over-exploration causes a serious issue. Thus, we conducted Monte Carlo Thompson sampling (MTS) to balance the exploration-exploitation trade-off by defining the posterior distribution via the Monte Carlo method and compared its performance with TS in antibody engineering. Our results demonstrated that MTS largely outperforms TS in discovering desirable candidates at an earlier round when over-exploration occurs on TS. Thus, the MTS method is a powerful technique for efficiently discovering antibodies with desired characteristics when the number of rounds is limited.",
      "publication": "mAbs",
      "date": "2023-12-31 2023-12-31",
      "doi": "10.1080/19420862.2023.2244214",
      "tags": [
        "machine learning",
        "protein engineering",
        "Antibody engineering",
        "Bayesian optimization",
        "Monte Carlo method",
        "Thompson sampling"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/7SVKN82C/Kakuzaki 等 - 2023 - Monte Carlo Thompson sampling-guided design for antibody engineering.pdf",
      "has_pdf": true,
      "pdf_pages": 11,
      "date_added": "2024-08-16T08:45:55",
      "scanned_at": "2026-01-06T09:32:40.883364",
      "pdf_loaded": false
    },
    {
      "id": "2KYU4N4I_27HRWFLE",
      "item_key": "2KYU4N4I",
      "title": "Bridging Model-Based Optimization and Generative Modeling via Conservative Fine-Tuning of Diffusion Models",
      "authors": "Masatoshi Uehara, Yulai Zhao, Ehsan Hajiramezanali, Gabriele Scalia, Gökcen Eraslan, Avantika Lal, Sergey Levine, Tommaso Biancalani",
      "abstract": "AI-driven design problems, such as DNA/protein sequence design, are commonly tackled from two angles: generative modeling, which efficiently captures the feasible design space (e.g., natural images or biological sequences), and model-based optimization, which utilizes reward models for extrapolation. To combine the strengths of both approaches, we adopt a hybrid method that fine-tunes cutting-edge diffusion models by optimizing reward models through RL. Although prior work has explored similar avenues, they primarily focus on scenarios where accurate reward models are accessible. In contrast, we concentrate on an offline setting where a reward model is unknown, and we must learn from static offline datasets, a common scenario in scientific domains. In offline scenarios, existing approaches tend to suffer from overoptimization, as they may be misled by the reward model in out-of-distribution regions. To address this, we introduce a conservative fine-tuning approach, BRAID, by optimizing a conservative reward model, which includes additional penalization outside of offline data distributions. Through empirical and theoretical analysis, we demonstrate the capability of our approach to outperform the best designs in offline data, leveraging the extrapolation capabilities of reward models while avoiding the generation of invalid designs through pre-trained diffusion models.",
      "publication": null,
      "date": "2024-05-31 2024-05-31",
      "doi": "10.48550/arXiv.2405.19673",
      "tags": [
        "Computer Science - Machine Learning",
        "Computer Science - Artificial Intelligence",
        "Statistics - Machine Learning"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/27HRWFLE/Uehara 等 - 2024 - Bridging Model-Based Optimization and Generative Modeling via Conservative Fine-Tuning of Diffusion.pdf",
      "has_pdf": true,
      "pdf_pages": 30,
      "date_added": "2024-08-23T14:17:15",
      "scanned_at": "2026-01-06T09:32:40.883369",
      "pdf_loaded": false
    },
    {
      "id": "2WQLQV4T_5RA5GPSM",
      "item_key": "2WQLQV4T",
      "title": "Diffusion Models as Constrained Samplers for Optimization with Unknown Constraints",
      "authors": "Lingkai Kong, Yuanqi Du, Wenhao Mu, Kirill Neklyudov, Valentin De Bortoli, Haorui Wang, Dongxia Wu, Aaron Ferber, Yi-An Ma, Carla P. Gomes, Chao Zhang",
      "abstract": "Addressing real-world optimization problems becomes particularly challenging when analytic objective functions or constraints are unavailable. While numerous studies have addressed the issue of unknown objectives, limited research has focused on scenarios where feasibility constraints are not given explicitly. Overlooking these constraints can lead to spurious solutions that are unrealistic in practice. To deal with such unknown constraints, we propose to perform optimization within the data manifold using diffusion models. To constrain the optimization process to the data manifold, we reformulate the original optimization problem as a sampling problem from the product of the Boltzmann distribution defined by the objective function and the data distribution learned by the diffusion model. To enhance sampling efficiency, we propose a two-stage framework that begins with a guided diffusion process for warm-up, followed by a Langevin dynamics stage for further correction. Theoretical analysis shows that the initial stage results in a distribution focused on feasible solutions, thereby providing a better initialization for the later stage. Comprehensive experiments on a synthetic dataset, six real-world black-box optimization datasets, and a multi-objective optimization dataset show that our method achieves better or comparable performance with previous state-of-the-art baselines.",
      "publication": null,
      "date": "2024-04-29 2024-04-29",
      "doi": null,
      "tags": [
        "Computer Science - Machine Learning",
        "Computer Science - Artificial Intelligence"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/5RA5GPSM/Kong 等 - 2024 - Diffusion Models as Constrained Samplers for Optimization with Unknown Constraints.pdf",
      "has_pdf": true,
      "pdf_pages": 21,
      "date_added": "2024-08-23T10:06:35",
      "scanned_at": "2026-01-06T09:32:40.883373",
      "pdf_loaded": false
    },
    {
      "id": "JQD4GMYZ_3BLR778K",
      "item_key": "JQD4GMYZ",
      "title": "A survey and benchmark of high-dimensional Bayesian optimization of discrete sequences",
      "authors": "Miguel González-Duque, Richard Michael, Simon Bartels, Yevgen Zainchkovskyy, Søren Hauberg, Wouter Boomsma",
      "abstract": "Optimizing discrete black-box functions is key in several domains, e.g. protein engineering and drug design. Due to the lack of gradient information and the need for sample efficiency, Bayesian optimization is an ideal candidate for these tasks. Several methods for high-dimensional continuous and categorical Bayesian optimization have been proposed recently. However, our survey of the field reveals highly heterogeneous experimental set-ups across methods and technical barriers for the replicability and application of published algorithms to real-world tasks. To address these issues, we develop a unified framework to test a vast array of high-dimensional Bayesian optimization methods and a collection of standardized black-box functions representing real-world application domains in chemistry and biology. These two components of the benchmark are each supported by flexible, scalable, and easily extendable software libraries (poli and poli-baselines), allowing practitioners to readily incorporate new optimization objectives or discrete optimizers. Project website: https://machinelearninglifescience.github.io/hdbo_benchmark",
      "publication": null,
      "date": "2024-11-04 2024-11-04",
      "doi": null,
      "tags": [
        "Computer Science - Machine Learning",
        "Statistics - Machine Learning"
      ],
      "pdf_path": null,
      "has_pdf": true,
      "pdf_pages": 0,
      "date_added": "2024-11-06T03:02:53",
      "scanned_at": "2026-01-06T09:32:40.883377",
      "pdf_loaded": false
    },
    {
      "id": "X5JELM5Z_EJCLHWZL",
      "item_key": "X5JELM5Z",
      "title": "Structure-Based Molecule Optimization via Gradient-Guided Bayesian Update",
      "authors": "Keyue Qiu, Yuxuan Song, Jie Yu, Hongbo Ma, Ziyao Cao, Zhilong Zhang, Yushuai Wu, Mingyue Zheng, Hao Zhou, Wei-Ying Ma",
      "abstract": "Structure-based molecule optimization (SBMO) aims to optimize molecules with both continuous coordinates and discrete types against protein targets. A promising direction is to exert gradient guidance on generative models given its remarkable success in images, but it is challenging to guide discrete data and risks inconsistencies between modalities. To this end, we leverage a continuous and differentiable space derived through Bayesian inference, presenting Molecule Joint Optimization (MolJO), the first gradient-based SBMO framework that facilitates joint guidance signals across different modalities while preserving SE(3)-equivariance. We introduce a novel backward correction strategy that optimizes within a sliding window of the past histories, allowing for a seamless trade-off between explore-and-exploit during optimization. Our proposed MolJO achieves state-of-the-art performance on CrossDocked2020 benchmark (Success Rate 51.3% , Vina Dock -9.05 and SA 0.78), more than 4x improvement in Success Rate compared to the gradient-based counterpart, and 2x \"Me-Better\" Ratio as much as 3D baselines. Furthermore, we extend MolJO to a wide range of optimization settings, including multi-objective optimization and challenging tasks in drug design such as R-group optimization and scaffold hopping, further underscoring its versatility and potential.",
      "publication": null,
      "date": "2024-11-21 2024-11-21",
      "doi": "10.48550/arXiv.2411.13280",
      "tags": [
        "Quantitative Biology - Biomolecules",
        "Computer Science - Artificial Intelligence"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/EJCLHWZL/Qiu 等 - 2024 - Structure-Based Molecule Optimization via Gradient-Guided Bayesian Update.pdf",
      "has_pdf": true,
      "pdf_pages": 27,
      "date_added": "2024-11-23T03:40:17",
      "scanned_at": "2026-01-06T09:32:40.883381",
      "pdf_loaded": false
    },
    {
      "id": "4FVMVWML",
      "item_key": "4FVMVWML",
      "title": "Does Structural Information Improve ESM3 for Protein Binding Affinity Prediction?",
      "authors": "Thomas Loux, Dianzhuo Wang, Eugene I Shakhnovich",
      "abstract": "This paper investigates the impact of incorporating structural information into the protein-protein interaction predictions made by ESM3, a multimodal protein language model (pLM). We utilized various structural variants as inputs and compared three widely used structure acquisition pipelines—EvoEF2, Gromacs, and Rosetta Relax—to assess their effects on ESM3’s performance. Our findings reveal that the use of a consistent identical structure, regardless of whether it is relaxed or variant, consistently enhances model performance across various datasets. This improvement is striking in few-show learning. However, performance deteriorates when different relaxed mutant structures are used for each variant. Based on these results, we advise caution when integrating distinct mutant structures into ESM3 and similar models.This study highlights the critical need for careful consideration of structural inputs in protein binding affinity prediction.",
      "publication": null,
      "date": null,
      "doi": null,
      "tags": [],
      "pdf_path": null,
      "has_pdf": false,
      "pdf_pages": 0,
      "date_added": "2024-12-22T15:02:20",
      "scanned_at": "2026-01-06T09:32:40.883386",
      "pdf_loaded": false
    },
    {
      "id": "AW4XHVY5_5CC9V4UY",
      "item_key": "AW4XHVY5",
      "title": "Improved Off-policy Reinforcement Learning in Biological Sequence Design",
      "authors": "Unknown",
      "abstract": "Designing biological sequences with desired properties is a significant challenge due to the combinatorially vast search space and the high cost of evaluating each candidate sequence. To address these challenges, reinforcement learning (RL) methods, such as GFlowNets, utilize proxy models for rapid reward evaluation and annotated data for policy training. Although these approaches have shown promise in generating diverse and novel sequences, the limited training data relative to the vast search space often leads to the misspecification of proxy for out-of-distribution inputs. We introduce $\\delta$-Conservative Search, a novel off-policy search method for training GFlowNets designed to improve robustness against proxy misspecification. The key idea is to incorporate conservativeness, controlled by parameter $\\delta$, to constrain the search to reliable regions. Specifically, we inject noise into high-score offline sequences by randomly masking tokens with a Bernoulli distribution of parameter $\\delta$ and then denoise masked tokens using the GFlowNet policy. Additionally, $\\delta$ is adaptively adjusted based on the uncertainty of the proxy model for each data point. This enables the reflection of proxy uncertainty to determine the level of conservativeness. Experimental results demonstrate that our method consistently outperforms existing machine learning methods in discovering high-score sequences across diverse tasks—including DNA, RNA, protein, and peptide design—especially in large-scale scenarios.",
      "publication": null,
      "date": "2024-10-04 2024/10/04",
      "doi": null,
      "tags": [
        "citationNumber: 0",
        "ccfInfo: CCF-None CORR"
      ],
      "pdf_path": null,
      "has_pdf": true,
      "pdf_pages": 0,
      "date_added": "2025-01-05T10:32:26",
      "scanned_at": "2026-01-06T09:32:40.883390",
      "pdf_loaded": false
    },
    {
      "id": "6HBJ9CU3_HW5EILHX",
      "item_key": "6HBJ9CU3",
      "title": "AlignAb: Pareto-Optimal Energy Alignment for Designing Nature-Like Antibodies",
      "authors": "Yibo Wen, Chenwei Xu, Jerry Yao-Chieh Hu, Han Liu",
      "abstract": "We present a three-stage framework for training deep learning models specializing in antibody sequence-structure co-design. We first pre-train a language model using millions of antibody sequence data. Then, we employ the learned representations to guide the training of a diffusion model for joint optimization over both sequence and structure of antibodies. During the final alignment stage, we optimize the model to favor antibodies with low repulsion and high attraction to the antigen binding site, enhancing the rationality and functionality of the designs. To mitigate conflicting energy preferences, we extend AbDPO (Antibody Direct Preference Optimization) to guide the model towards Pareto optimality under multiple energy-based alignment objectives. Furthermore, we adopt an iterative learning paradigm with temperature scaling, enabling the model to benefit from diverse online datasets without requiring additional data. In practice, our proposed methods achieve high stability and efficiency in producing a better Pareto front of antibody designs compared to top samples generated by baselines and previous alignment techniques. Through extensive experiments, we showcase the superior performance of our methods in generating nature-like antibodies with high binding affinity consistently.",
      "publication": null,
      "date": "2024-12-30 2024-12-30",
      "doi": "10.48550/arXiv.2412.20984",
      "tags": [
        "Computer Science - Machine Learning"
      ],
      "pdf_path": null,
      "has_pdf": true,
      "pdf_pages": 0,
      "date_added": "2025-01-05T08:31:35",
      "scanned_at": "2026-01-06T09:32:40.883395",
      "pdf_loaded": false
    },
    {
      "id": "LUFL4HLS_DV8BYMD9",
      "item_key": "LUFL4HLS",
      "title": "Designing diverse and high-performance proteins with a large language model in the loop",
      "authors": "Carlos A. Gomez-Uribe, Japheth Gado, Meiirbek Islamov",
      "abstract": "We present a novel protein engineering approach to directed evolution with machine learning that integrates a new semi-supervised neural network fitness prediction model, Seq2Fitness, and an innovative optimization algorithm, biphasic annealing for diverse adaptive sequence sampling (BADASS) to design sequences. Seq2Fitness leverages protein language models to predict fitness landscapes, combining evolutionary data with experimental labels, while BADASS efficiently explores these landscapes by dynamically adjusting temperature and mutation energies to prevent premature convergence and find diverse high-fitness sequences. Seq2Fitness predictions improve the Spearman correlation with fitness measurements over alternative model predictions, e.g., from 0.34 to 0.55 for sequences with mutations residues that are absent from the training set. BADASS requires less memory and computation compared to gradient-based Markov Chain Monte Carlo methods, while finding more higher-fitness sequences and maintaining sequence diversity in protein design tasks for two different protein families with hundreds of amino acids. For example, for both protein families 100% of the top 10,000 sequences found by BADASS have higher Seq2Fitness predictions than the wildtype sequence, versus a broad range between 3% to 99% for competing approaches with often many fewer than 10,000 sequences found. The fitness predictions for the top, top 100th, and top 1,000th sequences found by BADASS are all also higher. In addition, we developed a theoretical framework to explain where BADASS comes from, why it works, and how it behaves. Although we only evaluate BADASS here on amino acid sequences, it may be more broadly useful for exploration of other sequence spaces, including DNA and RNA. To ensure reproducibility and facilitate adoption, our code is publicly available here.\nAuthor summary Designing proteins with enhanced properties is essential for many applications, from industrial enzymes to therapeutic molecules. However, traditional protein engineering methods often fail to explore the vast sequence space effectively, partly due to the rarity of high-fitness sequences. In this work, we introduce BADASS, an optimization algorithm that samples sequences from a probability distribution with mutation energies and a temperature parameter that are updated dynamically, alternating between cooling and heating phases, to discover high-fitness proteins while maintaining sequence diversity. This stands in contrast to traditional approaches like simulated annealing, which often converge on fewer and lower fitness solutions, and gradient-based Markov Chain Monte Carlo (MCMC), also converging on lower fitness solutions and at a significantly higher computational and memory cost. Our approach requires only forward model evaluations and no gradient computations, enabling the rapid design of high-performing proteins that can be validated in the lab, especially when combined with our Seq2Fitness models. BADASS represents a significant advance in computational protein engineering, opening new possibilities for diverse applications.",
      "publication": null,
      "date": "2024-10-29 2024-10-29",
      "doi": "10.1101/2024.10.25.620340",
      "tags": [
        "ccfInfo: Not Found",
        "citationNumber: 0"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/DV8BYMD9/Gomez-Uribe 等 - 2024 - Designing diverse and high-performance proteins with a large language model in the loop.pdf",
      "has_pdf": true,
      "pdf_pages": 36,
      "date_added": "2025-01-19T02:52:05",
      "scanned_at": "2026-01-06T09:32:40.883400",
      "pdf_loaded": false
    },
    {
      "id": "FZMUJ9KT_W6ZKRBGD",
      "item_key": "FZMUJ9KT",
      "title": "Large Language Model is Secretly a Protein Sequence Optimizer",
      "authors": "Yinkai Wang, Jiaxing He, Yuanqi Du, Xiaohui Chen, Jianan Canal Li, Li-Ping Liu, Xiaolin Xu, Soha Hassoun",
      "abstract": "We consider the protein sequence engineering problem, which aims to find protein sequences with high fitness levels, starting from a given wild-type sequence. Directed evolution has been a dominating paradigm in this field which has an iterative process to generate variants and select via experimental feedback. We demonstrate large language models (LLMs), despite being trained on massive texts, are secretly protein sequence optimizers. With a directed evolutionary method, LLM can perform protein engineering through Pareto and experiment-budget constrained optimization, demonstrating success on both synthetic and experimental fitness landscapes.",
      "publication": null,
      "date": "2025-01-17 2025-01-17",
      "doi": "10.48550/arXiv.2501.09274",
      "tags": [
        "Computer Science - Machine Learning",
        "Computer Science - Artificial Intelligence",
        "Quantitative Biology - Quantitative Methods",
        "ccfInfo: Not Found",
        "citationNumber: 0"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/W6ZKRBGD/Wang 等 - 2025 - Large Language Model is Secretly a Protein Sequence Optimizer.pdf",
      "has_pdf": true,
      "pdf_pages": 12,
      "date_added": "2025-01-25T07:54:09",
      "scanned_at": "2026-01-06T09:32:40.883405",
      "pdf_loaded": false
    },
    {
      "id": "FIPU3HGK_6JNNNK4I",
      "item_key": "FIPU3HGK",
      "title": "Gradient-guided discrete walk-jump sampling for biological sequence generation",
      "authors": "Zarif Ikram, Dianbo Liu, M. Saifur Rahman",
      "abstract": "In this work, we propose gradient-guided discrete walk-jump sampling (gg-dWJS), a novel discrete sequence generation method for biological sequence optimization. Leveraging gradient guidance in the noisy manifold, we sample from the smoothed data manifold by applying discretized Markov chain Monte Carlo (MCMC) using a denoising model with the gradient-guidance from a discriminative model. This is followed by jumping to the discrete data manifold using a conditional one-step denoising. We showcase our method in two different modalities: discrete image and biological sequence involving antibody and peptide sequence generation tasks in the single objective and multi-objective setting. Through evaluation on these tasks, we show that our method generates high-quality samples that are well-optimized for specific tasks.",
      "publication": "Transactions on Machine Learning Research",
      "date": "2024-09-28 2024/09/28",
      "doi": null,
      "tags": [
        "ccfInfo: Not Found",
        "citationNumber: Not Found"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/6JNNNK4I/Ikram et al. - 2024 - Gradient-guided discrete walk-jump sampling for biological sequence generation.pdf",
      "has_pdf": true,
      "pdf_pages": 24,
      "date_added": "2025-02-17T14:05:28",
      "scanned_at": "2026-01-06T09:32:40.883409",
      "pdf_loaded": false
    },
    {
      "id": "L4UK5BBS_B4KJINBC",
      "item_key": "L4UK5BBS",
      "title": "Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics",
      "authors": "Jiahao Wang, Shuangjia Zheng",
      "abstract": "The ability to engineer optimized protein variants has transformative potential for biotechnology and medicine. Prior sequence-based optimization methods struggle with the high-dimensional complexities due to the epistasis effect and the disregard for structural constraints. To address this, we propose HADES, a Bayesian optimization method utilizing Hamiltonian dynamics to efficiently sample from a structure-aware approximated posterior. Leveraging momentum and uncertainty in the simulated physical movements, HADES enables rapid transition of proposals toward promising areas. A position discretization procedure is introduced to propose discrete protein sequences from such continuous state system. The posterior surrogate is powered by a two-stage encoder-decoder framework to determine the structure and function relationships between mutant neighbors, consequently learning a smoothed landscape to sample from. Extensive experiments demonstrate that our method outperforms state-of-the-art baselines in in-silico evaluations across most metrics. Remarkably, our approach offers a unique advantage by leveraging the mutual constraints between protein structure and sequence, facilitating the design of protein sequences with similar structures and optimized properties.",
      "publication": null,
      "date": "2024-10-04 2024/10/04",
      "doi": null,
      "tags": [
        "ccfInfo: Not Found",
        "citationNumber: Not Found"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/B4KJINBC/Wang and Zheng - 2024 - Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics.pdf",
      "has_pdf": true,
      "pdf_pages": 14,
      "date_added": "2025-02-17T14:04:30",
      "scanned_at": "2026-01-06T09:32:40.883414",
      "pdf_loaded": false
    },
    {
      "id": "FPZW9BZ5_6U8F34I7",
      "item_key": "FPZW9BZ5",
      "title": "GROOT: Effective Design of Biological Sequences with Limited Experimental Data",
      "authors": "Thanh V. T. Tran, Nhat Khang Ngo, Viet Anh Nguyen, Truong Son Hy",
      "abstract": "Latent space optimization (LSO) is a powerful method for designing discrete, high-dimensional biological sequences that maximize expensive black-box functions, such as wet lab experiments. This is accomplished by learning a latent space from available data and using a surrogate model to guide optimization algorithms toward optimal outputs. However, existing methods struggle when labeled data is limited, as training the surrogate model with few labeled data points can lead to subpar outputs, offering no advantage over the training data itself. We address this challenge by introducing GROOT, a Graph-based Latent Smoothing for Biological Sequence Optimization. In particular, GROOT generates pseudo-labels for neighbors sampled around the training latent embeddings. These pseudo-labels are then refined and smoothed by Label Propagation. Additionally, we theoretically and empirically justify our approach, demonstrate GROOT's ability to extrapolate to regions beyond the training set while maintaining reliability within an upper bound of their expected distances from the training regions. We evaluate GROOT on various biological sequence design tasks, including protein optimization (GFP and AAV) and three tasks with exact oracles from Design-Bench. The results demonstrate that GROOT equalizes and surpasses existing methods without requiring access to black-box oracles or vast amounts of labeled data, highlighting its practicality and effectiveness. We release our code at https://anonymous.4open.science/r/GROOT-D554",
      "publication": null,
      "date": "2024-11-18 2024-11-18",
      "doi": "10.48550/arXiv.2411.11265",
      "tags": [
        "Computer Science - Machine Learning",
        "Quantitative Biology - Quantitative Methods",
        "ccfInfo: Not Found",
        "citationNumber: 0"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/6U8F34I7/Tran et al. - 2024 - GROOT Effective Design of Biological Sequences with Limited Experimental Data.pdf",
      "has_pdf": true,
      "pdf_pages": 13,
      "date_added": "2025-02-17T14:03:19",
      "scanned_at": "2026-01-06T09:32:40.883420",
      "pdf_loaded": false
    },
    {
      "id": "2LRFYPC6_RKUCIVLQ",
      "item_key": "2LRFYPC6",
      "title": "Variational Search Distributions",
      "authors": "Daniel M. Steinberg, Rafael Oliveira, Cheng Soon Ong, Edwin V. Bonilla",
      "abstract": "We develop variational search distributions (VSD), a method for finding and generating discrete, combinatorial designs of a rare desired class in a batch sequential manner with a fixed experimental budget. We formalize the requirements and desiderata for active generation and formulate a solution via variational inference. In particular, VSD uses off-the-shelf gradient based optimization routines, can learn powerful generative models for designs, and can take advantage of scalable predictive models. We derive asymptotic convergence rates for learning the true conditional generative distribution of designs with certain configurations of our method. After illustrating the generative model on images, we empirically demonstrate that VSD can outperform existing baseline methods on a set of real sequence-design problems in various biological systems.",
      "publication": null,
      "date": "2024-12-07 2024-12-07",
      "doi": "10.48550/arXiv.2409.06142",
      "tags": [
        "Computer Science - Machine Learning",
        "Statistics - Machine Learning"
      ],
      "pdf_path": null,
      "has_pdf": true,
      "pdf_pages": 0,
      "date_added": "2025-02-17T14:02:48",
      "scanned_at": "2026-01-06T09:32:40.883423",
      "pdf_loaded": false
    },
    {
      "id": "Z68BYJB8_B7M4TWRD",
      "item_key": "Z68BYJB8",
      "title": "A Variational Perspective on Generative Protein Fitness Optimization",
      "authors": "Lea Bogensperger, Dominik Narnhofer, Ahmed Allam, Konrad Schindler, Michael Krauthammer",
      "abstract": "The goal of protein fitness optimization is to discover new protein variants with enhanced fitness for a given use. The vast search space and the sparsely populated fitness landscape, along with the discrete nature of protein sequences, pose significant challenges when trying to determine the gradient towards configurations with higher fitness. We introduce Variational Latent Generative Protein Optimization (VLGPO), a variational perspective on fitness optimization. Our method embeds protein sequences in a continuous latent space to enable efficient sampling from the fitness distribution and combines a (learned) flow matching prior over sequence mutations with a fitness predictor to guide optimization towards sequences with high fitness. VLGPO achieves state-of-the-art results on two different protein benchmarks of varying complexity. Moreover, the variational design with explicit prior and likelihood functions offers a flexible plug-and-play framework that can be easily customized to suit various protein design tasks.",
      "publication": null,
      "date": "2025-01-31 2025-01-31",
      "doi": "10.48550/arXiv.2501.19200",
      "tags": [
        "Computer Science - Machine Learning"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/B7M4TWRD/Bogensperger et al. - 2025 - A Variational Perspective on Generative Protein Fitness Optimization.pdf",
      "has_pdf": true,
      "pdf_pages": 12,
      "date_added": "2025-02-17T14:01:54",
      "scanned_at": "2026-01-06T09:32:40.883428",
      "pdf_loaded": false
    },
    {
      "id": "N522CPTU_NXDZ6GDI",
      "item_key": "N522CPTU",
      "title": "Protein Design with Guided Discrete Diffusion",
      "authors": "Nate Gruver, Samuel Don Stanton, Nathan C. Frey, Tim G. J. Rudner, Isidro Hotzel, Julien Lafrance-Vanasse, Arvind Rajpal, Kyunghyun Cho, Andrew Gordon Wilson",
      "abstract": "A popular approach to protein design is to combine a generative model with a discriminative model for conditional sampling. The generative model samples plausible sequences while the discriminative model guides a search for sequences with high fitness. Given its broad success in conditional sampling, classifier-guided diffusion modeling is a promising foundation for protein design, leading many to develop guided diffusion models for structure with inverse folding to recover sequences. In this work, we propose diffusioN Optimized Sampling (NOS), a guidance method for discrete diffusion models that follows gradients in the hidden states of the denoising network. NOS makes it possible to perform design directly in sequence space, circumventing significant limitations of structure-based methods, including scarce data and challenging inverse design. Moreover, we use NOS to generalize LaMBO, a Bayesian optimization procedure for sequence design that facilitates multiple objectives and edit-based constraints. The resulting method, LaMBO-2, enables discrete diffusions and stronger performance with limited edits through a novel application of saliency maps. We apply LaMBO-2 to a real-world protein design task, optimizing antibodies for higher expression yield and binding affinity to several therapeutic targets under locality and developability constraints, attaining a 99\\% expression rate and 40\\% binding rate in exploratory in vitro experiments.",
      "publication": null,
      "date": "2023-11-02 2023/11/02",
      "doi": null,
      "tags": [
        "ccfInfo: CCF-A NeurIPS",
        "citationNumber: 16"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/NXDZ6GDI/Gruver et al. - 2023 - Protein Design with Guided Discrete Diffusion.pdf",
      "has_pdf": true,
      "pdf_pages": 29,
      "date_added": "2025-02-17T13:58:41",
      "scanned_at": "2026-01-06T09:32:40.883432",
      "pdf_loaded": false
    },
    {
      "id": "QX8NFV6Q_2I7527FE",
      "item_key": "QX8NFV6Q",
      "title": "Optimizing protein fitness using Gibbs sampling with Graph-based Smoothing",
      "authors": "Andrew Kirjner, Jason Yim, Raman Samusevich, Tommi S. Jaakkola, Regina Barzilay, Ila R. Fiete",
      "abstract": "The ability to design novel proteins with higher fitness on a given task would be revolutionary for many fields of medicine. However, brute-force search through the combinatorially large space of sequences is infeasible. Prior methods constrain search to a small mutational radius from a reference sequence, but such heuristics drastically limit the design space. Our work seeks to remove the restriction on mutational distance while enabling efficient exploration. We propose Gibbs sampling with Graph-based Smoothing (GGS) which iteratively applies Gibbs with gradients to propose advantageous mutations using graph-based smoothing to remove noisy gradients that lead to false positives. Our method is state-of-the-art in discovering high-fitness proteins with up to 8 mutations from the training set. We study the GFP and AAV design problems, ablations, and baselines to elucidate the results. Code: https://github.com/kirjner/GGS",
      "publication": null,
      "date": "2023-08-02 2023/08/02",
      "doi": null,
      "tags": [
        "ccfInfo: CCF-None CORR",
        "citationNumber: 2"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/2I7527FE/Kirjner et al. - 2023 - Optimizing protein fitness using Gibbs sampling with Graph-based Smoothing.pdf",
      "has_pdf": true,
      "pdf_pages": 19,
      "date_added": "2025-02-17T13:57:41",
      "scanned_at": "2026-01-06T09:32:40.883438",
      "pdf_loaded": false
    },
    {
      "id": "FGF92P4R_295KMXP8",
      "item_key": "FGF92P4R",
      "title": "Self-supervised machine learning methods for protein design improve sampling but not the identification of high-fitness variants",
      "authors": "莫里茨 ·埃特尔特, 罗科· 莫雷蒂, 延斯· 梅勒, 克拉拉 T. Schoeder",
      "abstract": "抽象\n机器学习 （ML） 正在改变计算蛋白质设计的世界，数据驱动方法在实验成功方面超越了基于生物物理的方法。然而，它们通常作为案例研究进行报告，缺乏集成和标准化，因此很难客观地进行比较。在这项研究中，我们为在 Rosetta 软件框架内预测氨基酸概率的方法建立了一个简化且多样化的工具箱，允许对这些模型进行并排比较。随后，现有的蛋白质适应度景观被用于在真实的蛋白质设计环境中对新型 ML 方法进行基准测试。我们专注于蛋白质设计的传统问题：采样和评分。我们研究的一个主要发现是，ML 方法更擅长从有害突变中清除采样空间。然而，在没有模型微调的情况下对产生的突变进行评分显示，与使用 Rosetta 评分相比，没有明显的改善。我们得出的结论是，ML 现在补充而不是取代蛋白质设计中的生物物理方法。",
      "publication": "Science Advances",
      "date": "2025-00-00 2025",
      "doi": "10.1126/sciadv.adr7338",
      "tags": [
        "citationNumber: 0",
        "ccfInfo: Net Error: 0"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/295KMXP8/·埃特尔特 et al. - 2025 - Self-supervised machine learning methods for protein design improve sampling but not the identificat.pdf",
      "has_pdf": true,
      "pdf_pages": 12,
      "date_added": "2025-02-17T13:06:05",
      "scanned_at": "2026-01-06T09:32:40.883443",
      "pdf_loaded": false
    },
    {
      "id": "DATM49VU_ERAIZUDJ",
      "item_key": "DATM49VU",
      "title": "GeoEvoBuilder: A deep learning framework for efficient functional and thermostable protein design",
      "authors": "佳乐 刘, 汉天 游, 郑 郭, Qin Xu, Changsheng Zhang, 璐华 黎",
      "abstract": "抽象虽然深度学习具有先进的蛋白质序列和功能设计，但设计高活性和稳定的蛋白质仍然需要劳动密集型的迭代计算设计和实验。迫切需要能够直接生成具有所需特性的蛋白质序列的方法。在这里，我们介绍了 GeoEvoBuilder，这是一个先进的深度学习框架，它自适应地集成了蛋白质序列设计的结构和进化约束。GeoEvoBuilder 准确地概括了功能位点并生成了正确折叠的序列，并增强了活性和热稳定性。GeoEvoBuilder 已被应用于重新设计绿色荧光蛋白、谷胱甘肽过氧化物酶 4 （GPX4） 和二氢叶酸还原酶 （DHFR），产生具有显着提高热稳定性和活性的变体。值得注意的是，顶级 DHFR 设计的催化效率提高了 20 倍，热稳定性提高了 10 °C。晶体结构测定证实，设计的蛋白质形成了正确的结构。对 GPX4 变体中残基动态相关性的进一步分析提供了对远程位点如何调节酶活性的见解。与专注于单一突变及其与迭代设计和实验周期组合的传统方法不同，GeoEvoBuilder 探索了一个大的序列空间，可以在一次运行中成功设计超过 30% 的残基变化。GeoEvoBuilder 不仅为蛋白质工程提供了变革性工具，而且还可用于揭示蛋白质序列、结构、功能和进化之间的复杂关系。GeoEvoBuilder 可在 https://github.com/PKUliujl/GeoEvoBuilder 公开获得。",
      "publication": "Proceedings of the National Academy of Sciences",
      "date": "2025-10-14 2025-10-14",
      "doi": "10.1073/pnas.2504117122",
      "tags": [
        "/unread"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/ERAIZUDJ/刘 等 - 2025 - GeoEvoBuilder A deep learning framework for efficient functional and thermostable protein design.pdf",
      "has_pdf": true,
      "pdf_pages": 11,
      "date_added": "2025-10-12T09:24:51",
      "scanned_at": "2026-01-06T09:32:40.883447",
      "pdf_loaded": false
    },
    {
      "id": "3DTXJNZK_W53E8CV6",
      "item_key": "3DTXJNZK",
      "title": "Steering Protein Language Models",
      "authors": "Long-Kai Huang, Rongyi Zhu, Bing He, Jianhua Yao",
      "abstract": "Protein Language Models (PLMs), pre-trained on extensive evolutionary data from natural proteins, have emerged as indispensable tools for protein design. While powerful, PLMs often struggle to produce proteins with precisely specified functionalities or properties due to inherent challenges in controlling their outputs. In this work, we investigate the potential of Activation Steering, a technique originally developed for controlling text generation in Large Language Models (LLMs), to direct PLMs toward generating protein sequences with targeted properties. We propose a simple yet effective method that employs activation editing to steer PLM outputs, and extend this approach to protein optimization through a novel editing site identification module. Through comprehensive experiments on lysozyme-like sequence generation and optimization, we demonstrate that our methods can be seamlessly integrated into both auto-encoding and autoregressive PLMs without requiring additional training. These results highlight a promising direction for precise protein engineering using foundation models.",
      "publication": null,
      "date": "2025-09-12 2025-09-12",
      "doi": "10.48550/arXiv.2509.07983",
      "tags": [
        "Computer Science - Machine Learning",
        "Quantitative Biology - Biomolecules",
        "/unread"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/W53E8CV6/Huang 等 - 2025 - Steering Protein Language Models.pdf",
      "has_pdf": true,
      "pdf_pages": 15,
      "date_added": "2025-12-27T09:37:29",
      "scanned_at": "2026-01-06T09:32:40.883453",
      "pdf_loaded": false
    },
    {
      "id": "W23SGA9L_UKWIITZB",
      "item_key": "W23SGA9L",
      "title": "Protein language models are biased by unequal sequence sampling across the tree of life",
      "authors": "Frances Ding, Jacob Steinhardt",
      "abstract": "Abstract 抽象的\nProtein language models (pLMs) trained on large protein sequence databases have been used to understand disease and design novel proteins. In design tasks, the likelihood of a protein sequence under a pLM is often used as a proxy for protein fitness, so it is critical to understand what signals likelihoods capture. In this work we find that pLM likelihoods unintentionally encode a species bias: likelihoods of protein sequences from certain species are systematically higher, independent of the protein in question. We quantify this bias and show that it arises in large part because of unequal species representation in popular protein sequence databases. We further show that the bias can be detrimental for some protein design applications, such as enhancing thermostability. These results highlight the importance of understanding and curating pLM training data to mitigate biases and improve protein design capabilities in under-explored parts of sequence space.基于大型蛋白质序列数据库训练的蛋白质语言模型（pLM）已被用于理解疾病和设计新型蛋白质。在设计任务中，蛋白质序列在 pLM 下的似然性通常被用作蛋白质适应度的指标，因此理解似然性所捕捉到的信号至关重要。本研究发现，pLM 似然性无意中编码了一种物种偏好：来自某些物种的蛋白质序列的似然性系统性地更高，且与所讨论的蛋白质无关。我们量化了这种偏好，并表明其主要原因是常用蛋白质序列数据库中物种代表性的不均衡。我们进一步表明，这种偏好可能对某些蛋白质设计应用（例如提高热稳定性）产生不利影响。这些结果凸显了理解和整理 pLM 训练数据的重要性，以减轻偏好并提高在序列空间中尚未充分探索区域的蛋白质设计能力。",
      "publication": null,
      "date": "2024-03-12 2024-03-12",
      "doi": "10.1101/2024.03.07.584001",
      "tags": [
        "/unread"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/UKWIITZB/Ding和Steinhardt - 2024 - Protein language models are biased by unequal sequence sampling across the tree of life.pdf",
      "has_pdf": true,
      "pdf_pages": 24,
      "date_added": "2025-12-27T09:38:23",
      "scanned_at": "2026-01-06T09:32:40.883457",
      "pdf_loaded": false
    },
    {
      "id": "8PES5APB_VKCP3M7Y",
      "item_key": "8PES5APB",
      "title": "Sampling Protein Language Models for Functional Protein Design",
      "authors": "Jeremie Theddy Darmawan, Yarin Gal, Pascal Notin",
      "abstract": "Protein language models have emerged as powerful tools for learning rich representations of proteins, enhancing performance across various downstream tasks such as structure prediction, mutation effects prediction, and homology detection. Their ability to learn complex distributions over protein sequences also shows significant potential for designing novel and functional proteins, with broad applications in therapeutics, new materials, and sustainability. Given the vastness of the protein sequence space, efficient exploration methods are critical to the success of protein engineering efforts. However, the methodologies for effectively sampling from these models to achieve core protein design objectives remain underexplored and have predominantly relied on techniques initially developed for Natural Language Processing tasks. In this work, we first develop a comprehensive *in silico* protein design evaluation framework to systematically compare different sampling methods. After a thorough review of existing sampling strategies for language models, we introduce several approaches specifically tailored for protein design. We then evaluate these strategies using our *in silico* benchmark, investigating the effects of key hyperparameters and providing practical guidance on the relative strengths of each method depending on design objectives.",
      "publication": null,
      "date": "2025-07-21 2025/07/21",
      "doi": null,
      "tags": [
        "/unread"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/VKCP3M7Y/Darmawan 等 - 2025 - Sampling Protein Language Models for Functional Protein Design.pdf",
      "has_pdf": true,
      "pdf_pages": 25,
      "date_added": "2025-12-27T09:38:50",
      "scanned_at": "2026-01-06T09:32:40.883461",
      "pdf_loaded": false
    },
    {
      "id": "ZW2PSQDM_D5MAD4KE",
      "item_key": "ZW2PSQDM",
      "title": "SeedProteo: Accurate De Novo All-Atom Design of Protein Binders",
      "authors": "Wei Qu, Yiming Ma, Fei Ye, Chan Lu, Yi Zhou, Kexin Zhang, Lan Wang, Minrui Gui, Quanquan Gu",
      "abstract": "We present SeedProteo, a diffusion-based model for de novo all-atom protein design. We demonstrate how to repurpose a cutting-edge folding architecture into a powerful generative design framework by effectively integrating self-conditioning features. Extensive benchmarks highlight the model’s capabilities across two distinct tasks: in unconditional generation, SeedProteo exhibits superior length generalization and structural diversity, maintaining robustness for long sequences and complex topologies; in binder design, it achieves state-of-the-art performance among open-source methods, attaining the highest in-silico design success rates, structural diversity and novelty.",
      "publication": null,
      "date": "2025-12-30 2025-12-30",
      "doi": "10.48550/arXiv.2512.24192",
      "tags": [
        "Quantitative Biology - Biomolecules",
        "/unread"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/D5MAD4KE/Qu 等 - 2025 - SeedProteo Accurate De Novo All-Atom Design of Protein Binders.pdf",
      "has_pdf": true,
      "pdf_pages": 18,
      "date_added": "2026-01-03T10:38:47",
      "scanned_at": "2026-01-06T09:32:40.883465",
      "pdf_loaded": false
    },
    {
      "id": "YERFQ2NH_TMESDQ8V",
      "item_key": "YERFQ2NH",
      "title": "Stable de novo protein design via joint conformational landscape and sequence optimization",
      "authors": "Yehlin Cho, Justas Dauparas, Kotaro Tsuboyama, Gabriel J. Rocklin, Sergey Ovchinnikov",
      "abstract": "Generative protein modeling provides advanced tools for designing diverse protein sequences and structures. However, accurately modeling the conformational landscape and designing sequences remain critical challenges: ensuring that the designed sequence reliably folds into the target structure as its most stable conformation, and optimizing the sequence for a given suboptimal fixed input structure. In this study, we present a systematic analysis of jointly optimizing sequence-to-structure and structure-to-sequence mappings. This approach enables us to find optimal solutions for modeling the conformational landscape. We validate our approach with large-scale protein stability measurements, demonstrating that joint optimization is superior for designing stable proteins using a joint model (TrRosetta and TrMRF) and for achieving high accuracy in stability prediction when jointly modeling (half-masked ESMFold pLDDT + ESM2 Pseudo-likelihood). We further investigate features of sequences generated from the joint model and find that they exhibit higher frequencies of hydrophilic interactions, which may help maintain both secondary structure registry and pairing-features not captured by structure-to-sequence modeling alone.",
      "publication": "Nature Communications",
      "date": "2025-12-24 2025-12-24",
      "doi": "10.1038/s41467-025-66526-w",
      "tags": [
        "Protein folding",
        "/unread",
        "Computational models",
        "Computational biophysics"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/TMESDQ8V/Cho 等 - 2025 - Stable de novo protein design via joint conformational landscape and sequence optimization.pdf",
      "has_pdf": true,
      "pdf_pages": 10,
      "date_added": "2026-01-04T02:10:32",
      "scanned_at": "2026-01-06T09:32:40.883470",
      "pdf_loaded": false
    },
    {
      "id": "BWXEVG4C_FX9NDSTS",
      "item_key": "BWXEVG4C",
      "title": "Structure and evolution-guided design of minimal RNA-guided nucleases",
      "authors": "Petr Skopintsev, Isabel Esain-Garcia, Evan C. DeTurk, Peter H. Yoon, Zehan Zhou, Trevor Weiss, Maris Kamalu, Ajit Chamraj, Kenneth J. Loi, Conner J. Langeberg, Ron Boger, Hunter Nisonoff, Hannah M. Karp, LinXing Chen, Honglue Shi, Kamakshi Vohra, Jillian F. Banfield, Jamie H. D. Cate, Steven E. Jacobsen, Jennifer A. Doudna",
      "abstract": "The design of RNA-guided nucleases with properties not limited by evolution can expand programmable genome editing capabilities. However, generating diverse multi-domain proteins with robust enzymatic properties remains challenging. Here we use an artificial intelligence-driven strategy that couples structure-guided inverse protein folding with evolution-informed residue constraints to generate active, divergent variants of TnpB, a minimal CRISPR-Cas12-like nuclease. High-throughput functional screening of AI-generated variants yielded editors that retained or exceeded wild-type activity in bacterial, plant and human cells. Cryo-EM-based structure determination of the most divergent active variant revealed new stabilizing contacts in the RNA/DNA interfaces across conformational states, demonstrating the design potential of this approach. Together these results establish a strategy for creating non-natural RNA-guided nucleases and conformationally active nucleic acid binders, enlarging the designable protein space.\nOne-sentence abstract An evolution- and structure-conditioned model enables design of active RNA-guided nucleases with new nucleic acid contacts resolved by cryo-EM.",
      "publication": null,
      "date": "2025-12-08 2025-12-08",
      "doi": "10.64898/2025.12.08.692503",
      "tags": [
        "/unread"
      ],
      "pdf_path": "/Users/tjx/Zotero/storage/FX9NDSTS/Skopintsev 等 - 2025 - Structure and evolution-guided design of minimal RNA-guided nucleases.pdf",
      "has_pdf": true,
      "pdf_pages": 11,
      "date_added": "2026-01-05T07:51:12",
      "scanned_at": "2026-01-06T09:32:40.883475",
      "pdf_loaded": false
    }
  ]
}